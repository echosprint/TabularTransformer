{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TabularTransformer","text":""},{"location":"#welcome-to-tabulartransformers-documentation","title":"Welcome to TabularTransformer\u2019s documentation!","text":"<p>TabularTransformer is a lightweight, end-to-end deep learning framework built with PyTorch that harnesses the Transformer architecture's capabilities for the tabular domain. It is designed to be scalable and efficient with the following advantages:</p> <ul> <li>Streamlined workflow with no need for preprocessing or handling missing values.</li> <li>Unleashing the power of Transformer on tabular data domain.</li> <li>Native GPU support through PyTorch.</li> <li>Minimal APIs to get started quickly.</li> <li>Capable of handling large-scale data.</li> </ul>"},{"location":"#architecture-of-tabulartransformer","title":"Architecture of TabularTransformer","text":"<p>The model consists of three main parts:</p> <ul> <li> <p>Embedding Layer: Each column in the tabular data, designated as either <code>Categorical</code> or <code>Numerical</code>, undergoes a two-part embedding process. Each column scalar value is considered to have two components: a <code>class</code> component and a <code>value</code> component. The <code>class</code> component is embedded using a simple lookup embedding similar to token embedding in <code>LLM</code>, while the <code>value</code> component is mapped into an n-dim space using absolute position encodings, inspired by  Vaswani et al.'s paper \"Attention is All You Need\" though with slight modifications. These two embeddings are then combined through addition, forming the embedding of the tabular column feature with the shape of <code>(n_row, n_col, n_dim)</code>, preparing the input for further processing by the Transformer.</p> </li> <li> <p>Transformer: The core of the model is the Transformer, which consists of multiple layers designed to capture contextual relationships between column features. By processing the embedded features through attention mechanisms, the Transformer is able to model dependencies and interactions between different column features, enriching each feature with contextual information from the others.</p> </li> <li> <p>Multi-Layer Perceptron (MLP): Once the Transformer generates the contextual embeddings, they are compressed into a lower-dimensional space and concatenated before being fed into an MLP. Serving as the final component of the model, the MLP aggregates and processes these representations to produce the final output, which may be a regression or classification result, depending on the specific task.</p> </li> </ul> <p>These three components work together to encode tabular data in a way that captures the relationships and interactions between column features, ultimately producing a highly contextual embedding that enhances predictive performance.</p> <p>The diagram of the model is illustrated below.</p> <p></p>"},{"location":"embedding/","title":"Embedding Pipeline","text":"<p>Unleashing the power of Transformers on tabular data: Embedding is all you need.</p> <p>Transformers have revolutionized natural language processing by enabling models to capture complex relationships in sequential data. However, applying Transformers directly to tabular data has been challenging due to the unique characteristics of tabular datasets, such as heterogeneous feature types and missing values. One of the primary hurdles is the lack of well-constructed embeddings for tabular features.</p> <p>In this document, we present a comprehensive embedding pipeline designed to enhance Transformer performance on tabular data by effectively representing both categorical and numerical features.</p>"},{"location":"embedding/#sample-tabular-data","title":"Sample Tabular Data","text":"<p>Consider the following sample dataset, where [UNK] denotes missing or anomalous values:</p> Occupation Age City Income Engineer 30 [UNK] 500,000 Doctor [UNK] San Francisco 1,000,000 [UNK] 35 Los Angeles 200,000 Artist 35 Los Angeles [UNK] <p>This dataset includes both categorical features (Occupation, City) and numerical features (Age, Income), as well as missing values.</p>"},{"location":"embedding/#feature-types","title":"Feature Types","text":"<p>Each feature (column) in the dataset must be classified as one of the following types based on its semantic meaning:</p> <ul> <li>Numerical: Features representing continuous values (e.g., Age, Income).</li> <li>Categorical: Features representing discrete categories (e.g., Occupation, City).</li> </ul> <p>Whether a feature is considered <code>Numerical</code> or <code>Categorical</code> depends on your understanding and interpretation of the data.</p> <p>This classification is essential because it determines how each feature will be processed during embedding.</p>"},{"location":"embedding/#building-the-feature-vocabulary","title":"Building the Feature Vocabulary","text":"<p>To effectively embed the features, we first construct a Feature Vocabulary that maps each unique token in the tabular to a unique index. This vocabulary includes special tokens for missing values (<code>[UNK]</code>) and numerical values (<code>[num]</code>).</p>"},{"location":"embedding/#feature-vocabulary-table","title":"Feature Vocabulary Table","text":"Feature Index Occupation_[UNK] 0 Age_[UNK] 1 City_[UNK] 2 Income_[UNK] 3 Occupation_Engineer 4 Occupation_Doctor 5 Occupation_Artist 6 Age_[num] 7 City_San Francisco 8 City_Los Angeles 9 Income_[num] 10 <p>This vocabulary ensures that each unique feature token is associated with a unique index, which will be used in the embedding process.</p>"},{"location":"embedding/#feature-tokens-lookup","title":"Feature Tokens Lookup","text":"<p>Using the Feature Vocabulary, we convert the original tabular data into a table of Feature Tokens, where each token is represented by its corresponding index.</p>"},{"location":"embedding/#feature-tokens-table","title":"Feature Tokens Table","text":"Occupation Age City Income 4 7 2 10 5 1 8 10 0 7 9 10 6 7 9 3 <p>These indices will be used with an embedding layer (e.g., PyTorch's <code>nn.Embedding</code>) to obtain vector representations of the feature tokens.</p> <pre><code>feature_vocab_size = len(feature_vocabulary_table)\nembedding = nn.Embedding(feature_vocab_size, n_dim)\nfeature_token_embeddings = embedding(feature_tokens_table)\n# shape (n_row, n_col, n_dim)\n</code></pre>"},{"location":"embedding/#computing-feature-values","title":"Computing Feature Values","text":"<p>For each feature, we also compute a Feature Value, which captures the magnitude of the feature.</p>"},{"location":"embedding/#assigning-values-to-unk","title":"Assigning Values to [UNK]","text":"<ul> <li>Missing or Unknown Values (<code>[UNK]</code>): Assign a default value of <code>1.0</code>.</li> </ul>"},{"location":"embedding/#assigning-values-to-features","title":"Assigning Values to Features","text":"<ul> <li> <p>Categorical Features: Since they represent discrete categories, we assign a default value of <code>1.0</code> to all categorical features.</p> </li> <li> <p>Numerical Features:</p> <ul> <li> <p>Power Transform (Optional): If <code>apply_power_transform</code> is set to <code>True</code>, a power transform is applied to make the data more Gaussian-like before normalization. This can help stabilize variance and minimize skewness.</p> </li> <li> <p>Normalization: Numerical features are normalized using z-score normalization to ensure they have zero mean and unit variance.</p> </li> </ul> </li> </ul> <pre><code>def power_transform(value):\n    return -np.log1p(-value) if value &lt; 0 else np.log1p(value)\n</code></pre>"},{"location":"embedding/#feature-values-table","title":"Feature Values Table","text":"<p>After assigning default values to [UNK] and categorical features, and optionally applying power transformation and normalization to numerical features, we obtain the following feature values:</p> Occupation Age City Income 1.0 -1.1547 1.0 -0.1650 1.0 1.0 1.0 1.0722 1.0 0.5774 1.0 -0.9073 1.0 0.5774 1.0 1.0 <p>Note: The numerical values here are illustrative; in practice, you would compute the exact z-scores.</p>"},{"location":"embedding/#feature-values-embedding","title":"Feature Values Embedding","text":"<p>To represent the Feature Values as embeddings, we map each normalized scalar value to a high-dimensional vector using a method similar to Absolute Position Encoding used in Transformers.</p> <p>For each value \\(val\\), and for dimensions \\(i\\) in \\(0\\) to \\(\\frac{n_{dim}}{2}\u200b\u200b\u22121\\), we compute:</p> \\[ FVE_{(val, 2i)} = \\sin\\left(val \\times 10000^{\\frac{2i}{n_{dim}}}\\right) \\] \\[ FVE_{(val, 2i+1)} = \\cos\\left(val \\times 10000^{\\frac{2i}{n_{dim}}}\\right) \\] <p>This results in a vector of size \\(n_{dim}\\) that encodes the scalar value in a way that the model can understand.</p>"},{"location":"embedding/#combining-embeddings","title":"Combining Embeddings","text":"<p>After obtaining the embeddings for Feature Tokens and Feature Values, we have two tensors:</p> <ul> <li>Feature Token Embeddings: Shape \\((n_{row}, n_{col}, n_{dim})\\)</li> <li>Feature Value Embeddings: Shape \\((n_{row}, n_{col}, n_{dim})\\)</li> </ul> <p>We combine these embeddings by element-wise addition to form the final input tensor to the Transformer:</p> \\[ {Input}_{Transformer\u200b}=Feature Token Embeddings + Feature Value Embeddings \\] <p>This combined embedding incorporates both the identity of the feature (through the token embedding) and its value (through the value embedding), enabling the Transformer to effectively process tabular data.</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#why-do-i-need-tabulartransformer","title":"Why do I need TabularTransformer?","text":"<p>TabularTransformer provides a convenient deep learning framework specifically designed for tabular data domain. For deep learning practitioners, it\u2019s easy to get started with. In contrast, tree-based models use different methodologies that may not be as familiar. Additionally, TabularTransformer can offer competitive performance for large-scale tabular data.</p>"},{"location":"faq/#can-the-tabulartransformer-outperform-xgboost-or-lightgbm","title":"Can the TabularTransformer outperform XGBoost or LightGBM?","text":"<p>It depends on the dataset size and complexity. For datasets with fewer than 50,000 samples, tree-based models like XGBoost or LightGBM often perform better. However, when working with millions of samples, the TabularTransformer can potentially outperform them.</p> <p>The key to unlocking the full potential of the TabularTransformer lies in the quality and size of dataset. It's crucial to invest time\u2014about 90%\u2014into curating the data and deeply understanding the domain problem. Ensuring diversity in the tabular features and maintaining high data quality are essential, as hyperparameter optimization alone offers limited improvements in performance.</p>"},{"location":"faq/#would-the-tabulartransformer-help-me-win-a-kaggle-competition","title":"Would the TabularTransformer help me win a Kaggle competition?","text":"<p>It can certainly help, but winning a competition often involves some luck. To earn a medal, it's usually important to ensemble different models, rather than relying on a single one. A deep understanding of the competition task and data is critical, sometimes a good model alone isn't enough to secure a win.</p>"},{"location":"getting-started/","title":"Supervised training using tabular-transformer","text":"<p>This notebook demonstrates how to use the TabularTransformer to efficiently handle a tabular data prediction task. We will walk through data preparation, model training, and prediction using a sample dataset.</p> <p>TabularTransformer is an end-to-end training framework that processes raw data directly, eliminating the need for handcrafted features and complex preprocessing steps. It provides a competitive alternative to tree-based models for handling tabular data.</p> <p> </p> <p>Alternatively, you can check out the notebook to run it locally.</p> <p>Please note that the hyperparameters used here are not optimized and may be suboptimal.</p>"},{"location":"getting-started/#setup","title":"Setup","text":"<p>First, we need to install the <code>tabular-transformer</code> package.</p> <pre><code>%pip install tabular-transformer\n</code></pre> <p>Next, we import the necessary modules.</p> <pre><code>import tabular_transformer as ttf\nimport torch\n</code></pre>"},{"location":"getting-started/#data-preparation","title":"Data Preparation","text":"<p>We will use the Adult Income dataset for our prediction task, which aims to determine whether a person makes over $50K a year. The dataset contains 32.6k rows and 15 columns, with the label column named <code>income</code>.</p> <pre><code>income_dataset_path = ttf.prepare_income_dataset()\n</code></pre> <p>TabularTransformer treats each column feature as either <code>Categorical</code> or <code>Numerical</code>. Here, we need to specify which columns fall into each category. For convenience, one of the lists can be left empty. In the snippet below, <code>numerical_cols</code> will includes all columns that are not specified as categorical.</p> <pre><code>categorical_cols = [\n    'workclass', 'education',\n    'marital.status', 'occupation',\n    'relationship', 'race', 'sex',\n    'native.country', 'income']\n\n# all remaining columns are numerical\nnumerical_cols = []\n</code></pre> <p>To properly instruct <code>tabular-transformer</code> on how to handle the data, we define an instance of the <code>DataReader</code>. This instance specifies which columns are categorical, which are numerical, and handles other data-related settings such as file paths, label column, header presence, ID column, etc. </p> <pre><code>income_reader = ttf.DataReader(\n    file_path=income_dataset_path,\n    ensure_categorical_cols=categorical_cols,\n    ensure_numerical_cols=numerical_cols,\n    label='income',\n    header=True,\n    id=None,\n)\n</code></pre> <p>Optionally, we can split the data into training and testing sets if a testing set is not already available. This dataset contains a mix of numerical, categorical, and missing features. <code>tabular-transformer</code> processes the data as is, so no preprocessing is required. Here, we split 20% of the data as the testing set and use the remaining 80% for training.</p> <pre><code>split = income_reader.split_data(\n    split={'test': 0.2, 'train': -1},\n    seed=42,\n    output_path='data/income/',\n    save_as='csv',\n)\nsplit\n</code></pre> <p>we create data readers for both the training and testing datasets using the <code>income_reader</code> instance.</p> <p>The <code>ttf.DataReader</code> instance is callable object, can be called to update specific attributes and return the updated instance. </p> <pre><code>train_data_reader = income_reader(file_path=split['train'])\ntest_data_reader = income_reader(file_path=split['test'])\n</code></pre>"},{"location":"getting-started/#model-training","title":"Model Training","text":"<p>Next, we specify the device for computation (CPU or GPU) and the data type to be used for training. We detect whether CUDA is available for acceleration; if not, we default to CPU.</p> <pre><code>device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndtype = 'bfloat16' if torch.cuda.is_available() \\\n    and torch.cuda.is_bf16_supported() else 'float16'\n</code></pre> <p>We set up the training settings using <code>ttf.TrainSettings</code>. These settings define various configurations that guide the model's training behavior:</p> <pre><code>ts = ttf.TrainSettings(\n    device=device,\n    dtype=dtype,\n    apply_power_transform=True,\n    min_cat_count=0.02,\n)\n</code></pre> <p>Explanation of Parameters:</p> <ul> <li> <p>device: Specifies the device (CPU or GPU) on which the training will be executed. Using a GPU (if available) can significantly speed up the training process.</p> </li> <li> <p>dtype: Defines the data type (e.g., torch.float32, torch.bfloat16) used during training, which can impact memory usage and computational performance.</p> </li> <li> <p>apply_power_transform: Indicates whether a power transformation should be applied to numerical features. This can help stabilize variance, making the data more suitable for training.</p> </li> <li> <p>min_cat_count: Sets the minimum count (as a proportion) for categorical values. Categories occurring less frequently than this threshold will be labeled as unknown, which helps in managing rare categories effectively.</p> </li> </ul> <p>Next, we define the model's hyperparameters using <code>ttf.HyperParameters</code>. These parameters determine the architecture and capacity of the TabularTransformer model:</p> <p><pre><code>hp = ttf.HyperParameters(\n    dim=64,\n    n_heads=8,\n    n_layers=6)\n</code></pre> Explanation of Hyperparameters:</p> <ul> <li> <p>dim: Sets the dimensionality of the embeddings in the Transformer. A value of 64 indicates that each embedding vector will have 64 dimensions, influencing the model's capacity to capture complex patterns in the data.</p> </li> <li> <p>n_heads: Specifies the number of attention heads in each multi-head attention layer. Using 8 heads allows the model to focus on different parts of the input sequence simultaneously, capturing diverse relationships within the data.</p> </li> <li> <p>n_layers: Defines the number of layers (or blocks) in the Transformer. Here, 6 layers provide the depth necessary for the model to learn intricate patterns in tabular data through sequential processing.</p> </li> </ul> <p>We then define the training parameters for the TabularTransformer model using <code>ttf.TrainParameters</code>. Each parameter is crucial in guiding the model's training process:</p> <pre><code>tp = ttf.TrainParameters(\n    max_iters=3000, learning_rate=5e-4,\n    output_dim=1, loss_type='BINCE',\n    batch_size=128, eval_interval=100,\n    eval_iters=20, warmup_iters=100,\n    validate_split=0.2)\n</code></pre> <p>Explanation of Parameters:</p> <ul> <li> <p>max_iters: Sets the maximum number of iterations for training, determining how many times the model will go through the training data.</p> </li> <li> <p>learning_rate: Controls the step size during model weight updates. A smaller value like 5e-4 ensures the model learns gradually, reducing the risk of overshooting the optimal solution.</p> </li> <li> <p>output_dim: Specifies the dimension of the model's output. Here, it is set to 1, which is typical for binary classification or regression tasks.</p> </li> <li> <p>loss_type: Indicates the loss function to be used during training. <code>BINCE</code> stands for Binary Cross-Entropy, commonly used for binary classification problems.</p> </li> <li> <p>batch_size: The number of samples processed in each iteration. A batch size of 128 balances the need for stable gradient estimates and computational efficiency.</p> </li> <li> <p>eval_interval: The model will be evaluated on the validation set every 100 iterations, allowing you to monitor its performance regularly during training.</p> </li> <li> <p>eval_iters: Defines the number of iterations used during the evaluation phase. It helps average the performance over several mini-batches to get a more stable evaluation metric.</p> </li> <li> <p>warmup_iters: Specifies a warm-up phase for the first 100 iterations where the learning rate gradually increases to its set value. This technique helps stabilize the initial training phase.</p> </li> <li> <p>validate_split: The proportion of the dataset reserved for validation. Here, 20% of the data will be used to validate the model's performance during training, ensuring that the model is not overfitting.</p> </li> </ul> <p>Finally, we create a <code>ttf.Trainer</code> instance and initiate the training process for the TabularTransformer model. We will train the model using a one-liner.</p> <pre><code>trainer = ttf.Trainer(hp=hp, ts=ts)\n\ntrainer.train(\n    data_reader=train_data_reader,\n    tp=tp)\n</code></pre> <p>Explanation:</p> <ul> <li> <p><code>trainer = ttf.Trainer(hp=hp, ts=ts)</code>: This line initializes the Trainer with the specified hyperparameters (<code>hp</code>) and training settings (<code>ts</code>). The Trainer is responsible for managing the training loop, optimization, and model evaluation.</p> </li> <li> <p><code>trainer.train(data_reader=train_data_reader, tp=tp)</code>: This line starts the training process. It takes the <code>train_data_reader</code> to load the training data and <code>tp</code> for the training parameters (e.g., learning rate, batch size). During training, the model iteratively processes the data, adjusts its weights, and learns patterns from the input features to improve its predictive performance.</p> </li> </ul>"},{"location":"getting-started/#making-predictions","title":"Making Predictions","text":"<p>After training the model, we use the trained model checkpoint to make predictions on the test data.</p> <p>The code below demonstrates how to use the trained model to make predictions on the test dataset:</p> <pre><code>predictor = ttf.Predictor(checkpoint='out/ckpt.pt')\n\nprediction = predictor.predict(\n    data_reader=test_data_reader,\n    save_as=\"prediction_income.csv\"\n)\n</code></pre> <p>Explanation:</p> <ul> <li> <p><code>predictor = ttf.Predictor(checkpoint='out/ckpt.pt')</code>: This line creates an instance of Predictor using the model checkpoint stored at 'out/ckpt.pt'. The checkpoint contains the trained model's parameters and train configurations, allowing us to make predictions on new data.</p> </li> <li> <p><code>prediction = predictor.predict(data_reader=test_data_reader, save_as=\"prediction_income.csv\")</code>: This line runs the prediction process using the <code>test_data_reader</code> to load and process the test data. The results are saved to a CSV file named <code>\"prediction_income.csv\"</code>. The <code>predict()</code> method generates predictions based on the patterns the model learned during training.</p> </li> </ul> <p>displays the first few rows of the prediction <code>pd.DataFrame</code></p> <pre><code>prediction.head(3)\n</code></pre>"},{"location":"getting-started/#conclusion","title":"Conclusion","text":"<p>In this notebook, we demonstrated the workflow of using the TabularTransformer for handling tabular data. You can experiment with different hyperparameters and datasets to explore the model's capabilities further.</p>"},{"location":"hyperparameters/","title":"HyperParameters","text":""},{"location":"hyperparameters/#hyperparameters","title":"HyperParameters","text":"<p>Hyperparameters for Transformer model and AdamW optimizer</p> <ul> <li>dim (int): Dimension of embedding. Default is 64.</li> <li>n_layers (int): Number of Transformer layers. Default is 6.</li> <li>n_heads (int): Number of attention heads. Default is 8.</li> <li>output_hidden_dim (int): Hidden layer dimension of output MLP head. Default is 128.</li> <li>output_forward_dim (int): Dimension to squeeze the embedding before concatenation. Default is 8.</li> <li>multiple_of (int): Hidden dimension will be a multiple of this value. Default is 32.</li> <li>dropout (float): Dropout ratio. Default is 0.0.</li> <li>weight_decay (float): Weight decay parameter in AdamW optimizer. Default is 0.1.</li> <li>beta1 (float): Beta1 parameter in AdamW optimizer. Default is 0.9.</li> <li>beta2 (float): Beta2 parameter in AdamW optimizer. Default is 0.95.</li> </ul>"},{"location":"hyperparameters/#trainsettings","title":"TrainSettings","text":"<p>Training settings and configurations.</p> <ul> <li>out_dir (str): Output directory for checkpoints and predictions. Default is \"out\".</li> <li>log_interval (int): Interval of iterations for logging to the terminal. Default is 1.</li> <li>eval_only (bool): If True, the script exits after the first evaluation. Default is False.</li> <li>wandb_log (bool): Enable logging with Weights &amp; Biases. Default is False.</li> <li>wandb_project (str): Weights &amp; Biases project name. Default is \"TabularTransformer\".</li> <li>wandb_run_name (str): Weights &amp; Biases run name. Default is \"run\".</li> <li>min_cat_count (float): Minimum category count for valid classes; others labeled as <code>UNKNOWN</code>. Default is 0.02.</li> <li>apply_power_transform (bool): Apply power transform to numerical columns. Default is True.</li> <li>unk_ratio_default (float): Default percentage of tabular values to be randomly masked as unknown during training. Default is 0.2.</li> <li>dataset_seed (int): Seed for dataset loader. Default is 42.</li> <li>torch_seed (int): Seed for PyTorch. Default is 1377.</li> <li>dataset_device (str): Device to load the dataset when tokenized. Default is \"cpu\".</li> <li>device (str): Training device (e.g., 'cpu', 'cuda'). Default is \"cuda\".</li> <li>dtype (Literal): PyTorch data type for training ('float32', 'bfloat16', 'float16'). Default is \"bfloat16\".</li> </ul>"},{"location":"hyperparameters/#trainparameters","title":"TrainParameters","text":"<p>Parameters for the training process.</p> <ul> <li>max_iters (int): Total number of training iterations. Default is 100000.</li> <li>batch_size (int): Batch size per iteration. Default is 128.</li> <li>output_dim (int): Output dimension of the model. Default is 1.</li> <li>loss_type (Literal): Type of loss function ('BINCE', 'MULCE', 'MSE', 'SUPCON').              <code>BINCE</code>: <code>torch.nn.functional.binary_cross_entropy_with_logits</code>,             <code>MULCE</code>: <code>torch.nn.functional.cross_entropy</code>,             <code>MSE</code>: <code>torch.nn.functional.mse_loss</code>,             <code>SUPCON</code>: <code>Supervised Contrastive Loss</code>, see arXiv:2004.11362,             Default is 'BINCE'.</li> <li>eval_interval (int): Interval of iterations to start an evaluation. Default is 100.</li> <li>eval_iters (int): Number of iterations to run during evaluation. Default is 100.</li> <li>validate_split (float): Proportion of training data used for validation. Default is 0.2.</li> <li>unk_ratio (Dict[str, float]): Unknown ratio for specific columns, overrides <code>unk_ratio_default</code>. Default is <code>{}</code>.</li> <li>learning_rate (float): Learning rate for the optimizer. Default is 5e-4.</li> <li>transformer_lr (float): Learning rate for the transformer part; overrides <code>learning_rate</code> if set. Default is <code>None</code>.</li> <li>output_head_lr (float): Learning rate for the output head; overrides <code>learning_rate</code> if set. Default is <code>None</code>.</li> <li>warmup_iters (int): Number of iterations for learning rate warm-up. Default is 1000.</li> <li>lr_scheduler (Literal): Type of learning rate scheduler ('constant', 'cosine'). Default is 'cosine'.</li> <li>checkpoint (str): Checkpoint file name for saving and loading. Default is \"ckpt.pt\".</li> <li>input_checkpoint (str): Input checkpoint file for resuming training, overrides <code>checkpoint</code> if set.</li> <li>output_checkpoint (str): Output checkpoint file name for saving, overrides <code>checkpoint</code> if set.</li> </ul>"}]}