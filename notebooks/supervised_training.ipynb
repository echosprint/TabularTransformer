{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised training using [tabular-transformer](https://github.com/echosprint/TabularTransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use the TabularTransformer, a lightweight deep learning framework implemented with pure python, to handle tabular data prediction task efficiently. We will walk through data preparation, model training, and prediction using a sample dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TabularTransformer is an end-to-end training framework that processes data as-is, eliminating the cumbersome task of handcrafting intermediate features and complexity of preprocessing, providing an competent alternative of tree-based models dealing with tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/echosprint/TabularTransformer/blob/main/notebooks/supervised_training.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "you can check out the [notebook](https://github.com/echosprint/TabularTransformer/blob/main/notebooks/supervised_training.ipynb) to run locally.\n",
    "\n",
    "\n",
    "**for more details about the [TabularTransformer](https://github.com/echosprint/TabularTransformer) model**,\n",
    "ckeck the online **[Documents](https://echosprint.github.io/TabularTransformer/)**\n",
    "\n",
    "\n",
    "- This notebook provides a usage example of the\n",
    "  [TabularTransformer](https://github.com/echosprint/TabularTransformer)\n",
    "  package.\n",
    "- Hyperparameters are not tuned and may be suboptimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, we need to install the `tabular-transformer` packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tabular-transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabular_transformer as ttf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "first we need prepare the tabular dataset, here we use [Adult income dataset](https://huggingface.co/datasets/scikit-learn/adult-census-income) for convenience. The prediction task is to determine whether a person makes over $50K a year. The dataset has\n",
    "32.6k rows and 15 columns, the label column is `income`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_dataset_path = ttf.prepare_income_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TabularTransformer treats each column feature as either `Categorical` or `Numerical`. Here, we need to specify which columns fall into each category. For convenience, one of the lists can be left empty. In the snippet below, `numerical_cols` will includes all columns that are not specified as categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    'workclass', 'education',\n",
    "    'marital.status', 'occupation',\n",
    "    'relationship', 'race', 'sex',\n",
    "    'native.country', 'income']\n",
    "\n",
    "# all the rest columns are numerical, no need listed explicitly\n",
    "numerical_cols = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly instruct `tabular_transformer` on how to handle the data, we define an instance of the `DataReader`. This instance specifies which columns are categorical, which are numerical, and handles other data-related settings such as file paths, label column, whether header exists, id column etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_reader = ttf.DataReader(\n",
    "    file_path=income_dataset_path,\n",
    "    ensure_categorical_cols=categorical_cols,\n",
    "    ensure_numerical_cols=numerical_cols,\n",
    "    label='income',\n",
    "    header=True,\n",
    "    id=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally we can split into training and testing sets if testing set is not available yet.\n",
    "\n",
    "This dataset contains a mix of numeric, categorical and missing features. tabular-transformer treats the data as it is, and no preprocessing is required.\n",
    "\n",
    "Here we split the 20% of data as testing set, the rest as training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = income_reader.split_data(\n",
    "    split={'test': 0.2, 'train': -1},\n",
    "    seed=42,\n",
    "    output_path='data/income/',\n",
    "    save_as='csv',\n",
    ")\n",
    "split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create data readers for both the training and testing datasets using the `income_reader` instance.\n",
    "\n",
    "The `ttf.DataReader` instance is callable object, can be called to update specific attributes and return the updated instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_reader = income_reader(file_path=split['train'])\n",
    "test_data_reader = income_reader(file_path=split['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part involves specifying the device for computation (CPU or GPU) and the data type to be used for training.\n",
    "\n",
    "Detect whether `gpu` cuda is available for accelerating, if not, default to `cpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() \\\n",
    "    and torch.cuda.is_bf16_supported() else 'float16'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters for model and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets up the training settings using `ttf.TrainSettings`. These settings define various configurations that guide the model's training behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = ttf.TrainSettings(\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    "    apply_power_transform=True,\n",
    "    min_cat_count=0.02,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of Parameters:\n",
    "\n",
    "   - device=device: Specifies the device (CPU or GPU) on which the training will be executed. Using a GPU (if available) can significantly speed up the training process.\n",
    "\n",
    "   - dtype=dtype: Defines the data type (e.g., torch.float32, torch.bfloat16) used during training, which can impact memory usage and computational performance.\n",
    "\n",
    "   - apply_power_transform=True: Indicates whether a power transformation should be applied to numerical features. This can help stabilize variance, making the data more suitable for training.\n",
    "\n",
    "   - min_cat_count=0.02: Sets the minimum count (as a proportion) for categorical values. Categories occurring less frequently than this threshold will be labeled as `unknown`, which helps in managing rare categories effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is Transformer and MLP under the hood, specify the model hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code snippet below, we define the model's hyperparameters using `ttf.HyperParameters`. These parameters determine the architecture and capacity of the `TabularTransformer` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = ttf.HyperParameters(\n",
    "    dim=64,\n",
    "    n_heads=8,\n",
    "    n_layers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of `Hyperparameters`:\n",
    "\n",
    "   - dim=64: This sets the dimensionality of the embeddings in the Transformer. A value of 64 indicates that each embedding vector will have 64 dimensions, which influences the model's capacity to capture complex patterns in the data.\n",
    "\n",
    "   - n_heads=8: Specifies the number of attention heads in each multi-head attention layer. Using 8 heads allows the model to focus on different parts of the input sequence simultaneously, capturing diverse relationships within the data.\n",
    "\n",
    "   - n_layers=6: Defines the number of layers (or blocks) in the Transformer. Here, 6 layers provide the depth necessary for the model to learn intricate patterns in tabular data through sequential processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define the training parameters for the `TabularTransformer` model using `ttf.TrainParameters`. Each parameter is crucial in guiding the model's training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = ttf.TrainParameters(\n",
    "    max_iters=3000, learning_rate=5e-4,\n",
    "    output_dim=1, loss_type='BINCE',\n",
    "    batch_size=128, eval_interval=100,\n",
    "    eval_iters=20, warmup_iters=100,\n",
    "    validate_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of Parameters:\n",
    "\n",
    "  -  max_iters=3000: This sets the maximum number of iterations for training. It determines how many times the model will go through the training data.\n",
    "\n",
    "  - learning_rate=5e-4: The learning rate controls the step size during model weight updates. A smaller value like 5e-4 ensures the model learns gradually, reducing the risk of overshooting the optimal solution.\n",
    "\n",
    "  - output_dim=1: Specifies the dimension of the model's output. Here, it is set to 1, which is typical for binary classification or regression tasks.\n",
    "\n",
    "  - loss_type='BINCE': Indicates the loss function to be used during training. 'BINCE' stands for Binary Cross-Entropy, commonly used for binary classification problems.\n",
    "\n",
    "  - batch_size=128: The number of samples processed in each iteration. A batch size of 128 balances the need for stable gradient estimates and computational efficiency.\n",
    "\n",
    "  - eval_interval=100: The model will be evaluated on the validation set every 100 iterations, allowing you to monitor its performance regularly during training.\n",
    "\n",
    "  - eval_iters=20: Defines the number of iterations used during the evaluation phase. It helps average the performance over several mini-batches to get a more stable evaluation metric.\n",
    "\n",
    "  - warmup_iters=100: Specifies a warm-up phase for the first 100 iterations where the learning rate gradually increases to its set value. This technique helps stabilize the initial training phase.\n",
    "\n",
    "  - validate_split=0.2: The proportion of the dataset reserved for validation. Here, 20% of the data will be used to validate the model's performance during training, ensuring that the model is not overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create a `ttf.Trainer` instance and initiate the training process for the `TabularTransformer` model.\n",
    "\n",
    "We will train the model using a one-liner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ttf.Trainer(hp=hp, ts=ts)\n",
    "\n",
    "trainer.train(\n",
    "    data_reader=train_data_reader,\n",
    "    tp=tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "   - `trainer = ttf.Trainer(hp=hp, ts=ts)`: This line initializes the Trainer with the specified hyperparameters (`hp`) and training settings (`ts`). The Trainer is responsible for managing the training loop, optimization, and model evaluation.\n",
    "\n",
    "   - `trainer.train(data_reader=train_data_reader, tp=tp)`: This line starts the training process. It takes the `train_data_reader` to load the training data and `tp` for the training parameters (e.g., learning rate, batch size). During training, the model iteratively processes the data, adjusts its weights, and learns patterns from the input features to improve its predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "After training the model, we use the trained model checkpoint to make predictions on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below demonstrates how to use the trained model to make predictions on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = ttf.Predictor(checkpoint='out/ckpt.pt')\n",
    "\n",
    "prediction = predictor.predict(\n",
    "    data_reader=test_data_reader,\n",
    "    save_as=\"prediction_income.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "   - `predictor = ttf.Predictor(checkpoint='out/ckpt.pt')`: This line creates an instance of Predictor using the model checkpoint stored at 'out/ckpt.pt'. The checkpoint contains the trained model's parameters and train configurations, allowing us to make predictions on new data.\n",
    "\n",
    "   - `prediction = predictor.predict(data_reader=test_data_reader, save_as=\"prediction_income.csv\")`: This line runs the prediction process using the `test_data_reader` to load and process the test data. The results are saved to a CSV file named `\"prediction_income.csv\"`. The `predict()` method generates predictions based on the patterns the model learned during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " displays the first few rows of the prediction `pd.DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we demonstrated the workflow of using the TabularTransformer for handling tabular data. You can experiment with different hyperparameters and datasets to explore the model's capabilities further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabular",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
